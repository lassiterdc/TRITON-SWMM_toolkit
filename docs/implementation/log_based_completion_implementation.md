# Implementation: Log-File-Based Completion Checking

**Date:** 2026-02-07
**Status:** ✅ Complete

## Summary

Successfully migrated simulation completion checking from CFG-file-existence-based to log-file-based detection. This resolves the critical issue where `clear_raw_outputs=True` was deleting CFG checkpoints needed for completion verification, causing consolidation failures.

## Problem Statement

### Original Issue (PC04 Test Failure)

**Symptom:**
- Multi-model workflows with `clear_raw_outputs=True` failed during consolidation
- Error: "Scenarios not run" even though simulations completed successfully
- Processing step deleted CFG checkpoint files that completion checking relied on

**Root Cause:**
- `_check_triton_or_tritonswmm_simulation_run_status()` verified completion by checking for CFG files in `output/cfg/*.cfg`
- Processing step with `clear_raw_outputs=True` deleted these CFG files
- Consolidation then couldn't verify completion → reported scenarios as "not run"

**Race Condition (Multi-Model Exacerbating Factor):**
- TRITON-SWMM had legacy `simlog` for completion tracking
- But when TRITON-only, TRITON-SWMM, and SWMM-only ran concurrently, they could overwrite each other's simlog entries
- Simlog proved unreliable for multi-model scenarios

## Solution: Log-File-Based Completion Markers

### Key Insight

Simulation log files (`run_triton.log`, `run_tritonswmm.log`, `run_swmm.log`) are:
1. **Persistent** - Survive cleanup operations (not deleted with raw outputs)
2. **Model-specific** - Each model has its own log file (no race conditions)
3. **Already contain completion markers** - TRITON prints "Simulation ends", SWMM prints "EPA SWMM completed"

### Implementation

#### 1. Rewrote `model_run_completed()` Method

**File:** `src/TRITON_SWMM_toolkit/run_simulation.py` (lines 61-118)

```python
def model_run_completed(
    self, model_type: Literal["triton", "tritonswmm", "swmm"]
) -> bool:
    """Check if a simulation completed for a specific model type.

    Uses log file markers as source of truth:
    - TRITON/TRITON-SWMM: "Simulation ends" in run_{model}.log
    - SWMM: "EPA SWMM completed" in run_swmm.log
    """
    log_dir = self._scenario.scen_paths.logs_dir
    if not log_dir:
        return False

    if model_type == "triton":
        log_file = log_dir / "run_triton.log"
    elif model_type == "tritonswmm":
        log_file = log_dir / "run_tritonswmm.log"
    elif model_type == "swmm":
        log_file = log_dir / "run_swmm.log"

    if not log_file.exists():
        return False

    try:
        log_content = log_file.read_text()

        if model_type in ("triton", "tritonswmm"):
            # TRITON completion marker (may have ANSI color codes)
            return "Simulation ends" in log_content
        else:  # swmm
            # SWMM completion marker
            return "EPA SWMM completed" in log_content

    except Exception:
        return False
```

**Benefits:**
- ✅ Survives `clear_raw_outputs=True`
- ✅ No race conditions (model-specific files)
- ✅ Simple substring search (handles ANSI codes naturally)
- ✅ Fail-safe (returns False on any error)

#### 2. Separated Hotstart Retrieval from Completion Checking

**File:** `src/TRITON_SWMM_toolkit/run_simulation.py`

**Before:**
```python
status, f_last_cfg = self._check_triton_or_tritonswmm_simulation_run_status(...)
if status == "simulation completed":
    return
if status == "simulation started but did not finish":
    cfg = f_last_cfg
```

**After:**
```python
# Renamed: _check_triton_or_tritonswmm_simulation_run_status
#       → _retrieve_hotstart_file_for_incomplete_triton_or_tritonswmm_simulation

# Check completion first (via log file)
if self._scenario.model_run_completed(model_type):
    return None

# Then try hotstart if requested
if pickup_where_leftoff and model_type != "swmm":
    hotstart_cfg = self._retrieve_hotstart_file_for_incomplete_triton_or_tritonswmm_simulation(...)
    if hotstart_cfg is not None:
        cfg = hotstart_cfg
        # Resume from checkpoint
```

**Key Changes:**
- Completion checking NO LONGER depends on CFG file existence
- Hotstart retrieval is separate concern (returns `Path | None`)
- Prevents attempting to read CFG files that were cleaned up

#### 3. Added Performance File Sanity Check

**File:** `src/TRITON_SWMM_toolkit/scenario.py` (lines 267-285)

```python
def model_run_completed(self, model_type: Literal["triton", "tritonswmm", "swmm"]) -> bool:
    """Check completion status for a specific model type."""
    success = self.run.model_run_completed(model_type)

    # Sanity check for TRITON/TRITON-SWMM: performance.txt should only exist if completed
    if model_type in ("triton", "tritonswmm"):
        perf_file = self.run.performance_file(model_type=model_type)
        if perf_file.exists() and not success:
            raise RuntimeError(
                f"{model_type} simulation has ambiguous completion status:\n"
                f"  - performance.txt exists: {perf_file}\n"
                f"  - Log-based check says: NOT completed\n"
                f"This indicates completion detection needs strengthening."
            )

    return success
```

**Purpose:** Catch cases where completion detection is broken (performance.txt implies success, but log says failure).

#### 4. Deprecated Simlog Tracking

**Files:**
- `src/TRITON_SWMM_toolkit/log.py` - Kept `SimLog`, `SimEntry` classes but marked DEPRECATED
- `src/TRITON_SWMM_toolkit/run_simulation_runner.py` - Commented out all `add_sim_entry()` calls
- `src/TRITON_SWMM_toolkit/run_simulation.py` - Commented out simlog writes in obsolete methods

**Strategy:**
- Keep structures for backward compatibility (prevents errors loading old log files)
- Stop writing new entries (SimLog.update() doesn't call `_log.write()`)
- Mark properties like `sim_compute_time_min` as DEPRECATED (return placeholder values)

**Rationale:**
- Per CLAUDE.md: "Backward compatibility is NOT a priority"
- But keeping structures prevents immediate breakage
- Future cleanup can delete entirely once confirmed nothing depends on them

## Additional Fixes Discovered

### 1. Fixed `raw_triton_output_dir` Property → Method

**File:** `src/TRITON_SWMM_toolkit/run_simulation.py` (lines 46-70)

**Issue:** Pre-existing bug where `process_simulation.py` called `raw_triton_output_dir(model_type=...)` but it was defined as a property (no parameters).

**Fix:** Converted to method accepting `model_type` parameter:

```python
def raw_triton_output_dir(self, model_type: Literal["triton", "tritonswmm"] = "tritonswmm"):
    """Directory containing raw TRITON binary output files (H, QX, QY, MH)."""
    raw_type = self._analysis.cfg_analysis.TRITON_raw_output_type

    if model_type == "triton":
        base = self._scenario.scen_paths.out_triton
    else:
        base = self._scenario.scen_paths.out_tritonswmm

    if base is None:
        base = self._scenario.scen_paths.sim_folder / "output"

    raw_dir = base / raw_type
    if raw_dir.exists() and any(raw_dir.iterdir()):
        return raw_dir
    return base
```

### 2. Made `performance_file` Property → Method

**File:** `src/TRITON_SWMM_toolkit/run_simulation.py` (lines 120-144)

**Reason:** Needed for sanity check in `scenario.py` - must accept `model_type` parameter to check correct performance.txt file.

### 3. Fixed `sim_compute_time_min` to Handle Empty Simlog

**File:** `src/TRITON_SWMM_toolkit/scenario.py` (lines 227-257)

**Issue:** Property tried to iterate over empty `sim_log.run_attempts` dict, causing `ValueError: max() arg is an empty sequence`.

**Fix:** Return `0.0` placeholder since simlog no longer populated.

## Files Modified

| File | Changes |
|------|---------|
| `src/TRITON_SWMM_toolkit/run_simulation.py` | Rewrote `model_run_completed()` with log-file checking; renamed `_check_triton_or_tritonswmm_simulation_run_status` → `_retrieve_hotstart_file_for_incomplete_triton_or_tritonswmm_simulation`; updated `prepare_simulation_command()`; deleted `_check_swmm_simulation_run_status()`; fixed `raw_triton_output_dir` property→method; made `performance_file` a method |
| `src/TRITON_SWMM_toolkit/scenario.py` | Added performance file sanity check to `model_run_completed()`; deprecated `sim_compute_time_min` (returns 0.0); added docstring to `latest_simlog` |
| `src/TRITON_SWMM_toolkit/log.py` | Marked `SimLog`, `SimEntry` as DEPRECATED; stopped persisting writes (SimLog.update() no longer calls `_log.write()`) |
| `src/TRITON_SWMM_toolkit/run_simulation_runner.py` | Removed `add_sim_entry()` calls; simplified completion checking to use `model_run_completed()` |

## Testing Status

### ✅ Completed Implementation

All planned tasks completed:
1. ✅ Rewrite `model_run_completed()` with log-file checking
2. ✅ Rename and narrow CFG hotstart retrieval method
3. ✅ Update `prepare_simulation_command()` to use new methods
4. ✅ Add performance file sanity check to scenario.py
5. ✅ Delete obsolete simlog tracking code (deprecated, not deleted)
6. ✅ Update runner script to remove simlog calls

### ✅ Test Validation Complete

All tests pass, including `test_PC_04_multisim_with_snakemake.py::test_snakemake_workflow_end_to_end`.

The stale log reference bug that previously blocked test validation was resolved by the
model-specific logs implementation (commit d0e7b7a), which eliminated the shared `log.json`
race condition entirely. See `docs/archived/model_specific_logs_refactoring.md` for design.

## Benefits Achieved

1. **Robust Completion Detection**
   - No longer depends on files that get deleted
   - Survives aggressive cleanup operations
   - Single source of truth (log files)

2. **Multi-Model Support**
   - Each model has its own log file
   - No race conditions from concurrent execution
   - Clear separation of concerns

3. **Maintainability**
   - Simple substring search (easy to understand)
   - Fail-safe defaults (returns False on errors)
   - Consistent pattern across model types

4. **Aligned with Best Practices**
   - Per CLAUDE.md: "Prefer log-based checks over file existence"
   - Follows established `_already_written()` pattern (checks log field AND file)
   - Extends principle to completion checking

## Remaining Cleanup (Optional)

- Remove `SimLog`, `SimEntry` classes entirely (currently marked DEPRECATED)
- Delete commented-out simlog code in run_simulation.py
- Audit codebase for any remaining CFG-existence completion checks

## Lessons Learned

### ★ Pattern: Log Files > In-Memory Fields

**Problem:** In-memory log fields (simlog, LogField tracking) are prone to:
- Stale references after refresh operations
- Race conditions in concurrent execution
- Lost state after process restarts

**Solution:** Use persistent artifacts as source of truth:
- Log files for completion markers
- Actual output files for existence verification
- `_already_written()` combines both (file exists + log field set)

### ★ Separation of Concerns

**Anti-pattern:** Coupling completion checking with hotstart retrieval

**Better:**
```python
# Check completion (one concern)
if completed:
    return

# Retrieve hotstart (different concern)
if resume_requested:
    cfg = get_hotstart()
```

This prevents trying to read CFG files that don't exist and makes each operation's purpose clear.

### ★ Defense in Depth

**Belt-and-suspenders approach:**
1. Primary check: Log file contains completion marker
2. Secondary sanity check: Performance file exists → log must say complete
3. Tertiary check (via `_already_written`): File exists AND log field set

Multiple layers catch inconsistencies and make failures explicit.

## References

- Original plan: (conversation context)
- Bugfix plan: `docs/planning/bugfixes/log_field_stale_reference_fix.md`
- CLAUDE.md guidance: "Prefer log-based checks over file existence checks"
